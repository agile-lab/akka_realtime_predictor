akka {
  loglevel = INFO

  actor {
    provider = "akka.cluster.ClusterActorRefProvider"
    guardian-supervisor-strategy = "akka.actor.DefaultSupervisorStrategy"
    deployment {
      user/router-transactionWriter {
        router = round-robin-pool
        nr-of-instances = 5
      }
    }
  }

  remote {
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "127.0.0.1"
      port = 2551
    }
  }

  remote.watch-failure-detector.threshold = 20

  cluster {
    seed-nodes = [
      "akka.tcp://RTClassificationSystem@127.0.0.1:2551"
    ]

    auto-down-unreachable-after = 10s
  }
  persistence {
    journal {
      plugin = "cassandra-journal"
    }

    snapshot-size=100

    snapshot-store {
      plugin = "cassandra-snapshot-store"
    }

    at-least-once-delivery {
      # Interval between re-delivery attempts.
      redeliver-interval = 60s
      # Maximum number of unconfirmed messages that will be sent in one
      # re-delivery burst.
      redelivery-burst-limit = 10000
      # After this number of delivery attempts a
      # `ReliableRedelivery.UnconfirmedWarning`, message will be sent to the actor.
      warn-after-number-of-unconfirmed-attempts = 5
      # Maximum number of unconfirmed messages that an actor with
      # AtLeastOnceDelivery is allowed to hold in memory.
      max-unconfirmed-messages = 100000
    }
  }

  contrib.cluster.sharding {
    # The extension creates a top level actor with this name in top level user scope,
    # e.g. '/user/sharding'
    guardian-name = sharding
    # If the coordinator can't store state changes it will be stopped
    # and started again after this duration.
    coordinator-failure-backoff = 1 s
    # Start the coordinator singleton manager on members tagged with this role.
    # All members are used if undefined or empty.
    # ShardRegion actor is started in proxy only mode on nodes that are not tagged
    # with this role.
    role = ""
    # The ShardRegion retries registration and shard location requests to the
    # ShardCoordinator with this interval if it does not reply.
    retry-interval = 1 s
    # Maximum number of messages that are buffered by a ShardRegion actor.
    buffer-size = 10000
    # Timeout of the shard rebalancing process.
    handoff-timeout = 60 s
    # Time given to a region to acknowdge it's hosting a shard.
    shard-start-timeout = 10 s
    # If the shard can't store state changes it will retry the action
    # again after this duration. Any messages sent to an affected entry
    # will be buffered until the state change is processed
    shard-failure-backoff = 10 s
    # If the shard is remembering entries and an entry stops itself without
    # using passivate. The entry will be restarted after this duration or when
    # the next message for it is received, which ever occurs first.
    entry-restart-backoff = 10 s
    # Rebalance check is performed periodically with this interval.
    rebalance-interval = 10 s
    # How often the coordinator saves persistent snapshots, which are
    # used to reduce recovery times
    snapshot-interval = 3600 s
    # Setting for the default shard allocation strategy
    least-shard-allocation-strategy {
      # Threshold of how large the difference between most and least number of
      # allocated shards must be to begin the rebalancing.
      rebalance-threshold = 10
      # The number of ongoing rebalancing processes is limited to this number.
      max-simultaneous-rebalance = 3
    }
  }

}

spark-model {
  num-features = 500
  training-data-path = "data/training_set.gz"
  ndg-data-path = "data/ndg_set"
}
transactions-generator {
  interval = 1000
  time-unit = "ms"
}
cassandra-config {
  hosts = ["127.0.0.1:9042"]
}

cassandra-journal {

  # FQCN of the cassandra journal plugin
  class = "akka.persistence.cassandra.journal.CassandraJournal"

  # Comma-separated list of contact points in the cluster.
  # Host:Port pairs are also supported. In that case the port parameter will be ignored.
  contact-points = ${cassandra-config.hosts}

  cassandra-journal.max-message-batch-size = 10

}

cassandra-snapshot-store {

   # FQCN of the cassandra snapshot store plugin
     class = "akka.persistence.cassandra.snapshot.CassandraSnapshotStore"

   # Comma-separated list of contact points in the cluster.
   # Host:Port pairs are also supported. In that case the port parameter will be ignored.
   contact-points = ${cassandra-config.hosts}

   # replication strategy to use. SimpleStrategy or NetworkTopologyStrategy
   replication-strategy = "SimpleStrategy"

   # Replication factor to use when creating a keyspace. Is only used when replication-strategy is SimpleStrategy.
   replication-factor = 1

   # Write consistency level
   write-consistency = "ONE"

   # Read consistency level
   read-consistency = "ONE"

}
